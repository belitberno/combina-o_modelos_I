{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "666ba698",
   "metadata": {},
   "source": [
    "# Tarefa 02 - Módulo 23 -  Combinação de modelos I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ba9f7f",
   "metadata": {},
   "source": [
    "1. Monte um passo a passo para o algoritmo RF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e862912a",
   "metadata": {},
   "source": [
    "<img src=\"https://editor.analyticsvidhya.com/uploads/74060RF%20image.jpg\" width =\"800\" height=\"100\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963b7b35",
   "metadata": {},
   "source": [
    "[FONTE DA IMAGEM](https://editor.analyticsvidhya.com/uploads/74060RF%20image.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2623cbe",
   "metadata": {},
   "source": [
    "2. Explique com suas palavras o Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fd80df",
   "metadata": {},
   "source": [
    "<br> **Random Forest** é a técnica que denota o comportamento do algoritmo ao selecionar subconjuntos de features e montar mini árvores de decisão, não é atoa que o nome por si só já é explicativo, random significa aleatório, e forest significa floresta. Ou seja, essa técnica faz a seleção aleatória de algumas features, escolhe a featura mais adequada para a posição de nó raiz, gera os nós filhos e repete as ações acima até que se atinga a quantidade de árvores desejadas.<br>\n",
    "<br>Após a geração desses modelos, as previsãos são feitas a partir de votações, portanto, cada mini árvore toma uma decisão a parir dos dados apresentados, e a decisão mais votada e a resposta do algoritmo.<br>\n",
    "\n",
    "<br>As vantagens de **Random Forest** são:<br> \n",
    "- Resolvem problemas tanto de regressão, quanto de classificação. \n",
    "- Apresenta bons resultados em diversos tipos de problema. \n",
    "- Tem bom desempenho. \n",
    "- E as árvores dificilmente apontam overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21de0867",
   "metadata": {},
   "source": [
    "3. Qual a diferença entre Bagginge Random Forest?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a52f8c",
   "metadata": {},
   "source": [
    "<br>A diferença entre as duas técnicas seria que, o **Random Forest*** é uma melhoria da **Bagging** na forma em que as árvores são criadas, ou seja, descorrelacionando elas.<br>\n",
    "<br> Portanto no **Bagging**, enquanto utilizados os dados, e os mesmos podem retornar para ser utlizada em novos processos, modelos com alta variância entre si, e o predicto é a média desses modelos, a trativa do **Random Forest** é totalmente diferente, pois se buscam dados completamente aleatórios, para formar as pequenas árvores de decisão, cada árvore vota pelo seu predicto, e o qual tiver mais _\"votos\"_ é o _predicto escolhido_, assim, no **Bagging** podemos ver uma árvore com _overfitting_, no **Random Forest** já temos um cenário totalmente oposto.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2d5a18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
